{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let us execute the below cell to display information about the NVIDIA® CUDA® driver and the GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl+Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 [Optional]\n",
    "\n",
    "###  Learning objectives\n",
    "The **goal** of this lab is to:\n",
    "\n",
    "- Learn how to profile an application using NVIDIA Nsight™ Systems and Nsight Compute (CUDA example)\n",
    "\n",
    "We do not intend to cover:\n",
    "\n",
    "- How to use the CUDA programming model\n",
    "- How to implement shared memory\n",
    "- Optimization methods \n",
    "\n",
    "**NOTE: All the below screenshots are from an NVIDIA A100 Tensor Core GPU.**\n",
    "\n",
    "Please read the below notebooks before you start:\n",
    "\n",
    "- Overview of Nsight profiler tools ([Nsight Systems](nsight_systems.ipynb) and [Nsight Compute](nsight_compute.ipynb))\n",
    "\n",
    "In this lab, we will be porting the serial Jacobi code to the GPU using CUDA and obtaining the fastest performance possible. To achieve this, we need to consider Amdahl's law (please see the section on [Amdahl's Law and Scaling](profiling_lab1.ipynb#amdahls)) and remove as many performance limitations as possible. Common performance limiters are:\n",
    "\n",
    "- Serial portion of the code on the CPU (read the section on [Amdahl's Law and Scaling](profiling_lab1.ipynb#amdahls))\n",
    "- Memory movements (Device To/From Host)\n",
    "- Latency as a result of launching GPU kernels\n",
    "- Not enough work to hide instruction latency\n",
    "- Non-efficient memory access pattern\n",
    "    - Uncoalesced memory accesses, lack of cache reuse, not using shared memory (read more at [the NVIDIA Technical Blog](https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/) and [CUDA C Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/))\n",
    "- Low arithmetic intensity (the ratio between compute work [FLOPs] and data movement [bytes])\n",
    "\n",
    "### Step 0: Analyze the Code\n",
    "\n",
    "In this lab, we will be working on the Jacobi iteration code that iteratively converges to the correct value by computing new values at each point from the average of neighboring  points. \n",
    "\n",
    "<img src=\"images/jacobi_formula.png\" width=\"90%\">\n",
    "\n",
    "**Understand and analyze** the code present at:\n",
    " \n",
    "[Serial Code](../source_code/lab6/jacobi.cpp) \n",
    "\n",
    "To obtain the best performance from the GPU and utilize the hardware, one should follow the cyclical process (analyze, parallelize, optimize), explained in the [Introduction section](introduction.ipynb#steps). Start by compiling the code and running it on the CPU (**Note:** error will be printed periodically as output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Instrument the Code to Find Bottlenecks\n",
    "\n",
    "Before we start porting to the GPU, the first step is to identify the compute expensive part of the application to find the performance bottlenecks. One way to do this would be to use the CPU walltime to measure different parts of the code. Another method would be adding NVIDIA Tools Extension software development kit(NVTX). **Why are we using NVTX?** Please see the section on [Using NVIDIA Tools Extension (NVTX)](nsight_systems.ipynb#nvtx). We added the NVTX API to the code. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step1.cpp). (Reminder: we need to link it against the right runtime library [`libnvToolsExt.so`])\n",
    "\n",
    "Now, compile the code and **profile** with `nsys` (Reminder: `--stats=true` shows the summary output that includes information about time spent in the various NVTX regions.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step1 && nsys profile --stats=true -o jacobi_step1 -f true ./jacobi_step1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step1.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report via the Nsight Systems UI  locally. \n",
    "\n",
    "The timeline of the application is as shown below (feel free to hover your mouse over each section to see more detail; for example, the thread state).\n",
    "\n",
    "<img src=\"images/jacobi_1.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Using Unified Memory\n",
    "\n",
    "[Unified Memory(UM)](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/) is a single memory address space accessible from any processor in a system. It allows applications to allocate data that can be read or written from code running on either CPUs or GPUs. Now, to allocate Unified Memory, we replace calls to `malloc()` with calls to `cudaMallocManaged()` without making any other changes to the rest of the code. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step2.cpp). Now, compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step2 && nsys profile --stats=true -o jacobi_step2 -f true ./jacobi_step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step2.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. \n",
    "\n",
    "The timeline of the application is shown below.\n",
    "\n",
    "<img src=\"images/jacobi_2.png\" width=\"90%\">\n",
    "\n",
    "The application runs much longer than before. As you can see from the profiler report, there is a cost of initializing CUDA that can be high. In this example, this cost is much higher than the actual calculation time. \n",
    "\n",
    "<img src=\"images/jacobi_2_.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Make the Problem Bigger\n",
    "\n",
    "If we try to reduce the cost of computation and add fast kernels, it will still be slower than the CPU-only version (without UM). Like the previous lab, we simply need to make the problem bigger by either adding more particles/elements or increasing the iteration count. In this example, we increase the grid points `N` which achieves a finer spatial resolution and is more accurate. This will make the Jacobi relaxation step a big chunk of the total application runtime. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step3.cpp). Now,  compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step3 && nsys profile --stats=true -o jacobi_step3 -f true ./jacobi_step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step3.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. \n",
    "\n",
    "With a close look at the \"Timeline View\", we can see that the Jacobi relaxation step is now a big portion of the total application runtime.\n",
    "\n",
    "<img src=\"images/jacobi_3.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the most expensive portion of the application from the profiler report. Right click on the NVTX row from the \"Timline View\", then choose \"Show in Events View\". From the box at the bottom, you can sort all the ranges by duration. Now, you can see the `jacobi_step()` (Jacobi relaxation step) is the most compute expensive part of the code.\n",
    "\n",
    "<img src=\"images/jacobi_3_event.png\" width=\"90%\">\n",
    "\n",
    "### Step 4: Port the `jacobi_step` Kernel to GPU using CUDA\n",
    "\n",
    "We convert the `jacobi_step()` to a CUDA kernel, parallelizing over the inner and outer loop using a 2D threadblock of size `32x32`. Except for the `error`, each part can be updated independently `atomicadd()` to perform that. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step4.cpp). Now, let's compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step4 && nsys profile --stats=true -o jacobi_step4 -f true ./jacobi_step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step4.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. By checking the NVTX ranges in the \"Events View\", we can see that the \"Jacobi Step\" is significantly faster but the \"Swap data\" is slower than before.\n",
    "\n",
    "<img src=\"images/jacobi_4_.png\" width=\"40%\">\n",
    "\n",
    "If we look at the CUDA row, and zoom in, we see all those gaps that are marked as \"Swap data\" regions on the NVTX row. There is a lot of data movement from host to device in `jacobi_step()` kernel and device to host in the `swap_data()` function (due to the Unified Memory usage).\n",
    "\n",
    "<img src=\"images/jacobi_4_1.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Port the `swap_data` Kernel to GPU using CUDA \n",
    "\n",
    "We port the `swap_data()` function to the GPU using CUDA to do most of the computation on the device and reduce the data movement by keeping most of the data on the GPU. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step5.cpp). Now, compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step5 && nsys profile --stats=true -o jacobi_step5 -f true ./jacobi_step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step5.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. Check the NVTX ranges in the \"Events View\". We can see that the application is much faster now (e.g. \"Swap data\" is now faster than before). \n",
    "\n",
    "<img src=\"images/jacobi_5_0.png\" width=\"40%\">\n",
    "\n",
    "The timeline of the application is shown below. \n",
    "\n",
    "<img src=\"images/jacobi_5_1.png\" width=\"80%\">\n",
    "\n",
    "By looking at the CUDA row, it is clear that most of the application runtime is now spent in kernels and the data movement (green rectangles) is also reduced. Let's dig deeper and analyze the kernels in [Nsight Compute](nsight_compute.ipynb) to see if there are any other optimizations we can apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Analyze and Improve the Kernels\n",
    "\n",
    "In this section, we want to find out if the kernel is compute-bound, memory-bound, or latency-bound. To ensure the kernel is not limited by latency, we need to expose enough work to keep a large number of threads running on the GPU. In this example, `N` is equal to 2048, which means the problem size is `2048 x 2048`. Since the order of magnitude number of threads a modern GPU can run simultaneously is O(100k), then we probably have enough work to keep the device busy. \n",
    "\n",
    "In the case of memory bound or compute bound, we need to think of arithmetic intensity which is the ratio between compute work (FLOPs) and data movement (bytes) expressed as Floating Point Operations per second (FLOP/S). If this ratio is an order of 10, then it would be compute bound. \n",
    "\n",
    "```cpp\n",
    "f[IDX(i,j)] = 0.25f * (f_old[IDX(i+1,j)] + f_old[IDX(i-1,j)] +\n",
    "                      f_old[IDX(i,j+1)] + f_old[IDX(i,j-1)]);\n",
    "```\n",
    "\n",
    "The `jacobi_step()` kernel computes four floating point operations (three adds and one multiply) and moves four words (each 4 bytes for single precision floating point), so the arithmetic intensity is `4 (FLOPs) / 16 (bytes) = 0.25 FLOPs / byte` and it seems to be in the memory bandwidth region.\n",
    "\n",
    "Now, profile the kernel with Nsight Compute to verify our hypotheses. Make sure to read the [Nsight Compute](nsight_compute.ipynb) notebook before you continue. \n",
    "\n",
    "We only profile one invocation of the kernel since it has similar performance characteristics, and to allow the device to warmp up we skip the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:jacobi -f -o jacobi_step5_0 --set full ./jacobi_step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step5_0.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. \n",
    "\n",
    "The Speed of Light Throughput section gives a high-level overview of the throughput for the compute and memory resources of the GPU for each unit. Based on this information we can find the performance limiters and catagorize them into four possible combinations:\n",
    "\n",
    "- Compute Bound: SM>50% & Mem<50%\n",
    "- Bandwidth Bound: SM<50% & Mem>50%\n",
    "- Latency Bound: SM<50% & Mem<50%\n",
    "- Compute and Bandwidth Bound : SM>50% & Mem>50%\n",
    "\n",
    "According to the *GPU Speed of Light Throughput* section (see screenshot below), the kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of the device it was run on and since Since achieved compute throughput and/or memory bandwidth is below 60.0% of peak, the kernel is latency bound. The tool suggests looking at the *Scheduler Statistics* and *Warp State Statistics* for possible reasons. \n",
    "\n",
    "<img src=\"images/jacobi_5_sol.png\" width=\"80%\">\n",
    "\n",
    "The *Scheduler Statistics* shows the activity summary of the schedulers issuing instructions. Every scheduler can issue one instruction per cycle, but for this kernel, each scheduler only issues an instruction every ~1287 cycles. This causes underutilization of the hardware resources and leads to less  than optimal performance. \n",
    "\n",
    "<img src=\"images/jacobi_5_schedule.png\" width=\"80%\">\n",
    "\n",
    "Check the *Warp State Statistics* section  which presents an analysis  of the states of all warps that spent cycles during the kernel execution. The warp states describe a warp's readiness or inability to issue its next instruction. As seen in the screenshot below, the most important stall reason is *LG (local/global) Throttle*, which indicates extremely frequent memory instructions, according to the guided analysis rule (hover your mouse over each to see the description).\n",
    "\n",
    "<img src=\"images/jacobi_5_warp.png\" width=\"80%\">\n",
    "\n",
    "When we check the *Source Counters* section for the top stall locations in your source based on sampling data, we can see the kernel has uncoalesced global accesses (note: you can click on the links and go to the line where these occur). \n",
    "\n",
    "<img src=\"images/jacobi_5_source.png\" width=\"80%\">\n",
    "\n",
    "Last, looking at the *Occupancy* section, we achieved 71% occupancy which is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. In other words, we are giving enough work to the device to keep it busy. So, the low memory throughput is a result of poor memory access patterns.\n",
    "\n",
    "<img src=\"images/jacobi_5_occ.png\" width=\"80%\">\n",
    "\n",
    "To conclude, the reason for low memory throughput is poor memory access patterns which means the memory layout of our arrays does not map well with the threads (read more at [Analysis-Driven Optimization: Analyzing and Improving Performance with NVIDIA Nsight Compute](https://developer.nvidia.com/blog/analysis-driven-optimization-analyzing-and-improving-performance-with-nvidia-nsight-compute-part-2/)). Now, profile the `swap_data()` kernel with Nsight Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:swap_data -f -o jacobi_step5_1 --set full ./jacobi_step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step5_1.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. \n",
    "\n",
    "According to the *GPU Speed of Light Throughput* section (see screenshot below), the memory is more heavily utilized than compute and we need to look at the *Memory Workload Analysis* to identify the L2 bottleneck. \n",
    "\n",
    "<img src=\"images/jacobi_swap_sol.png\" width=\"80%\">\n",
    "\n",
    "The *Memory Workload Analysis* section tells us that the kernel has a poor memory access pattern. Rather than accessing 4 bytes per thread per memory request, there are `32x32 = 1023` bytes of cache data transfers per request which is 8 times more sector loads.\n",
    "\n",
    "<img src=\"images/jacobi_swap_mem.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the indexing scheme we used to the threading scheme. In a 2D threadblock, the `x` dimension is the contiguous dimension and the `y` dimension is the strided dimension. So, in a 32x32 thread block, each warp comprises of 32 threads in the `x` dimension and the `threadIdx.y` counts/enumerates 32 warps. To fix the uncoalessed memory access, we change `#define IDX(i, j) ((j) + (i) * N)` to `#define IDX(i, j) ((i) + (j) * N)`. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step6.cpp). \n",
    "\n",
    "Now, compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step6 && nsys profile --stats=true -o jacobi_step6 -f true ./jacobi_step6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step6.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. We can clearly see we made some overall improvements and reduced the total execution time.\n",
    "\n",
    "Lets profile the `swap_data` kernel with Nsight Compute to see if we fixed the global memory access pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:swap_data -f -o jacobi_step6_1 --set full ./jacobi_step6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step6_1.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. When comparing the `swap_data` kernel with the previous step, we can see that we fixed the memory access pattern and improved the overall performance.\n",
    "\n",
    "<img src=\"images/jacobi_6_swap_base.png\" width=\"80%\">\n",
    "\n",
    "We are now latency bound and if you look at the *Source Counters* section, we no longer have any noncoalesced global accesses. Now,  profile the `jacobi_step` kernel with Nsight Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:jacobi -f -o jacobi_step6_0 --set full --import-source 1 ./jacobi_step6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step6_0.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. Comparing to the previous version of the code, the `jacobi_step` kernel did achieve a substantial improvement.\n",
    "\n",
    "<img src=\"images/jacobi_6_jacobi_base.png\" width=\"80%\">\n",
    "\n",
    "Looking at the *Scheduler Statistics*, we see 99.9% of the cycles have no warps eligible to issue work. On cycles with no eligible warps, the issue slot is skipped and no instruction is issued. Having many skipped issue slots indicates poor latency hiding. \n",
    "\n",
    "<img src=\"images/jacobi_6_jacobi_schedule.png\" width=\"80%\">\n",
    "\n",
    "The reason for this is the atomic update to the error counter inside the `jacobi_step()` kernel (`atomicAdd(error, df * df)`). If multiple threads write to the same location at the same time, they will serialize and stall. We can refactor the kernel in a way to use a more efficient reduction scheme that uses fewer overall atomics. Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step7.cpp). Now, let's compile and profile the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step7 && nsys profile --stats=true -o jacobi_step7 -f true ./jacobi_step7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step7.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. \n",
    "\n",
    "We managed to reduce the total execution time enormously. Profile the `jacobi_step()` kernel with Nsight Compute to investigate more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:jacobi -f -o jacobi_step7_0 --set full --import-source 1 ./jacobi_step7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step7_0.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. \n",
    "\n",
    "<img src=\"images/jacobi_7_sol.png\" width=\"80%\">\n",
    "\n",
    "The *GPU Speed of Light* section shows we increase both compute and memory utilization compared to the previous section. Moreover, the Roofline charts shows an improvement in performance and an increase in arithmetic intensity.\n",
    "\n",
    "<img src=\"images/jacobi_7_roof.png\" width=\"80%\">\n",
    "\n",
    "Comparing the `jacobi_step` and `swap_data` kernel, one has more dynamic random-access memory (DRAM) throughput than the `jacobi_step` kernel. \n",
    "\n",
    "<img src=\"images/jacobi_7_base.png\" width=\"80%\">\n",
    "\n",
    "### Step 7: Shared Memory\n",
    "Although the code runs faster, there are still uncoalesced accesses that we need to address. The issue with the stencil operations is the cache reuse and as the array (problem size) grows, due to the stride between row `j` and row `j+1` , there will be less cache reuse. To solve this issue, we can read the data from the shared memory which is closer to the GPU cores. This will result in accessing the DRAM less. We implement the simplest version for ease but this might not be the best solution. To learn more on how to understand and optimize shared memory accesses using Nsight Compute, please watch the [GTC talk](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41723/).\n",
    "\n",
    "Feel free to inspect the [Jacobi Code](../source_code/lab6/jacobi_step8.cpp). Now, compile and profile the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && make jacobi_step8 && nsys profile --stats=true -o jacobi_step8 -f true ./jacobi_step8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step8.nsys-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Systems UI. \n",
    "\n",
    "Profile the kernel with Nsight Compute and inspect it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code/lab6 && ncu --launch-count 1 --launch-skip 5 -k --regex:jacobi --export jacobi_step8_0 --force-overwrite --set full ./jacobi_step8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../source_code/lab6/jacobi_step8_0.ncu-rep), then choosing <mark>Save Link As</mark>. Once done open the report locally via the Nsight Compute UI. \n",
    "\n",
    "<img src=\"images/jacobi_8_sol.png\" width=\"80%\">\n",
    "\n",
    "We achieved better compute and memory utilization compared to the previous section and memory access patterns are fully coalesced.\n",
    "\n",
    "\n",
    "<img src=\"images/jacobi_8_source.png\" width=\"80%\">\n",
    "\n",
    "The Nsight Systems profiler report shows how fast the kernels are. However, take to account that you might have to increase the problem size again as once again the device warmup might be a performance limiter.\n",
    "\n",
    "<img src=\"images/jacobi_8_nsight.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommended you go to your browser's file menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well. You can also execute the following cell block to create a zip file of the files you have been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f _profiler_files.zip\n",
    "zip -r _profiler_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> [Here](../_profiler_files.zip), then choosing <mark>Save Link As</mark>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# <p style=\"text-align:center; border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\"> <a href=../_start_profiling.ipynb>HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"float:center\"> <a href=nsight_advanced.ipynb>NEXT</a></span> </p>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[NVIDIA Nsight Compute](https://docs.nvidia.com/nsight-compute/index.html)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System and Compute profiler outputs, please download the latest versions from below pages:\n",
    "\n",
    "- https://developer.nvidia.com/nsight-systems\n",
    "- https://developer.nvidia.com/nsight-compute\n",
    "\n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
