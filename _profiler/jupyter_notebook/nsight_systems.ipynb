{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let us execute the below cell to display information about the NVIDIA® CUDA® driver and the GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl+Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "The **goal** of this lab is to:\n",
    "\n",
    "- Learn an overview of the NVIDIA Nsight™ Systems tool\n",
    "- Learn how to view and interpret your profiling report via NVIDIA Nsight Systems\n",
    "- Learn how to profile your application using Nsight Systems command line interface (CLI) with NVIDIA Tools Extension SDK (NVTX) application programming interface  (API)\n",
    "- Learn how to find the performance limiters from the **Timeline** view of the tool\n",
    "\n",
    "We do not intend to cover:\n",
    "\n",
    "- Advanced optimization techniques in detail\n",
    "\n",
    "### Introduction to Nsight Systems \n",
    "Nsight Systems offers system-wide performance analysis in order to visualize an application’s algorithms, help identify optimization opportunities, and improve the performance of applications running on a system consisting of multiple CPUs and GPUs.\n",
    "\n",
    "#### Nsight Systems Timeline\n",
    "\n",
    "When your profiling run with Nsight Systems is finished and you open the report in the Nsight Systems graphical user interface (GUI), the first thing you see is the **Timeline**  view. It consists of several timeline rows that show how the application is interacting with the various system resources over time. The profile shown in the below image is from a deep learning application using TensorFlow. So we have a scenario, which focuses on GPU activity and  what might limit the GPU to run at full load. The intention of all NVIDIA tools is to get the most performance from  your GPU system.\n",
    "\n",
    "<img src=\"images/nsight_sys_tags.png\">\n",
    "\n",
    "Nsight Systems is packed with many features. A few of the features are highlighted in the above screenshot:\n",
    "\n",
    "- API tracing of CUDA libraries and deep learning frameworks\n",
    "- CPU utilization, CPU thread states and thread migration as well as CPU callstack sampling\n",
    "- Operating system (OS) runtime library calls\n",
    "- GPU activities (kernels and memory copies) as well as GPU metrics\n",
    "\n",
    "In the following section, we briefly go over some of the Nsight Systems features. To read more, review https://docs.nvidia.com/nsight-systems/UserGuide/index.html.\n",
    "\n",
    "- **CPU Cores Workload:** CPU rows help locate the CPU core's idle times. Each row shows how the process' threads utilize the CPU cores. Each core is shown in a different call and average utilization can be seen on each subrows.\n",
    "\n",
    "<img src=\"images/cpu_row.png\">\n",
    "\n",
    "- **CPU Thread Activity:** Thread rows show a detailed view of each thread's activity including OS runtime libraries usage, CUDA API calls, NVTX time ranges and events (if integrated into the application).\n",
    "\n",
    "<img src=\"images/thread_row.png\">\n",
    "\n",
    "- **CUDA API:** This row show traces of CUDA API calls on the OS thread. You can:\n",
    "    - See when kernels are dispatched\n",
    "    - See when memory operations are initiated\n",
    "    - Locate the corresponding CUDA workload on GPU\n",
    " \n",
    "<img src=\"images/cuda_api.png\"> \n",
    "\n",
    "- **GPU Utilization:** CUDA workloads rows display kernel and memory transfer activities. \n",
    "\n",
    "<img src=\"images/cuda_row_0.png\"> \n",
    "\n",
    "You can zoom in and  see the locations where GPU is underutilized.\n",
    "\n",
    "<img src=\"images/cuda_row_1.png\">  \n",
    "\n",
    "\n",
    "You can also see the CPU-GPU correlation by clicking on a CUDA API call to see the correlation with the underlying GPU activity on the CUDA row (highlighted in teal):\n",
    "\n",
    "<img src=\"images/correlation.png\">  \n",
    "\n",
    "### Profiling using Command Line Interface (CLI)\n",
    "To profile your application, you can either use the graphical user interface(GUI) or command line interface (CLI). During this lab, we will profile a mini-application using CLI.\n",
    "\n",
    "The Nsight Systems CLI is referred to as `nsys`, provides several different commands. A basic profiling session can be done via `nsys profile ./app`. Below are some of the useful switches for CLI profiling:\n",
    "\n",
    "- API tracing: `-t, --trace=cuda,nvtx,osrt,opengl`, other options are `cublas`,`cusparse`,`cudnn`,`mpi`,`oshmem`,`ucx`,`openacc`,`openmp`,`vulkan`,`none`.\n",
    "- Overwrite existing report: `-f, --force-overwrite=[true|false]`\n",
    "- Summary statistics (profile output on command line): `--stats=[true|false]`\n",
    "- Report file name: `-o, --output=report#(patterns for hostname, PID and environment variables)`\n",
    "- Callstack sampling: `-s, --sample=[cpu|none]` , `--sampling-period=number of CPU Instructions Retired events` , `-b, --backtrace=[lbr|fp|dwarf|none]` , `--samples-per-backtrace={1..12} , (The number of CPU IP samples collected for every CPU IP sample backtrace collected.)`\n",
    "\n",
    "*Note:* Set the paranoid level: `“sudo sh -c 'echo 1 >/proc/sys/kernel/perf_event_paranoid’`\n",
    "\n",
    "- CUDA memory usage: `--cuda-memory-usage=[true|false]` , tracks the GPU memory usage by CUDA kernels. This is applicable only when CUDA tracing is enabled.\n",
    "\n",
    "**Note**: You do not need to memorize the profiler options. You can always run `nsys --help` or `nsys [specific command] --help` from the command line and use the necessary options or profiler arguments. Moreover, there is a special option to help with the transition from the legacy NVIDIA nvporf tool. Calling `nsys nvprof [options]` will provide the best available translation of `nvprof [options]`. For more information on Nsight Systems and NVTX, please see the __[Profiler documentation](https://docs.nvidia.com/nsight-systems/)__. \n",
    "\n",
    "An example of typical command line invocation: `nsys profile -t openacc,nvtx --stats=true --force-overwrite true -o laplace ./laplace`\n",
    "\n",
    "where command switch options used are:\n",
    "- `profile`: start a profiling session\n",
    "- `-t`: Selects the APIs to be traced (`nvtx` and `openacc` in this example)\n",
    "- `--stats`: if true, it generates a summary of statistics after the collection\n",
    "- `--force-overwrite`: if true, it overwrites the existing generated report\n",
    "- `-o`: name for the intermediate result file, created at the end of the collection (.qdrep filename)\n",
    "\n",
    "<!--\n",
    "*CLI Profiling - MPI Program:*\n",
    "\n",
    "- Single Node: `nsys profile [nsys_args] mpirun [mpirun_args] your_executable`. The command will create one report file.\n",
    "- Multiple Nodes: `mpirun [mpirun_args] nsys profile [nsys_args] your_executable`, you can set output report name with `-o report_name_%q{OMPI_COMM_WORLD_RANK}`. (For OpenMPI, PMI_RANK for MPICH and SLURM_PROCID for Slurm). The command will create one report file per MPI rank.\n",
    "\n",
    "You can also profile only specific ranks: \n",
    "```\n",
    "#!/bin/bash\n",
    "# OMPI_COMM_WORLD_LOCAL_RANK for node local rank\n",
    "if [ $OMPI_COMM_WORLD_RANK -eq 0 ]; then\n",
    "    nsys profile -t mpi \"$@\"\n",
    "else\n",
    "    \"$@\"\n",
    "fi\n",
    "```\n",
    "-->\n",
    "#### How to View the Report\n",
    "<a name=\"gui-report\"></a>\n",
    "When using CLI to profile the application, there are two ways to view the profiler's report. \n",
    "\n",
    "1) On the Terminal using the `--stats` option: By using `--stats` switch option, profiling results are displayed on the console terminal after the profiling data is collected. The collected data includes CUDA API, kernels and memory operations (by time and by size), OS runtime and NVTX.\n",
    "\n",
    "<img src=\"images/laplas3.png\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "2) NVIDIA Nsight System GUI: After the profiling session ends, a `*.nsys-rep` file will be created. This file can be loaded into Nsight Systems GUI using *File -> Open*. If you would like to view this on your local machine, this requires that the local system has the CUDA toolkit installed of the same version and the Nsight System GUI version should match the CLI version. More details on where to download the NVIDIA Nsight Systems can be found in the **Links and Resources** at the end of this page.\n",
    "\n",
    "To view the profiler report, simply open the file from the GUI (File > Open).\n",
    "\n",
    "<img src=\"images/nsight_open.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NVIDIA Tools Extension (NVTX) \n",
    "<a name=\"nvtx\"></a>\n",
    "NVIDIA Tools Extension (NVTX) is a C-based Application Programming Interface (API) for annotating events, time ranges and resources in applications. NVTX brings the profiled application’s logic into the Profiler, making the Profiler’s displayed data easier to analyze and enabling correlation of the displayed data to profiled application’s actions.  \n",
    "\n",
    "During this lab, we profile the application using the Nsight Systems CLI and collect the timeline. We will also be tracing NVTX APIs (already integrated into the application). The NVTX tool is a powerful mechanism that allows users to manually instrument their application. NVIDIA Nsight Systems can then collect the information and present it on the timeline. It is particularly useful for tracing CPU events and time ranges and greatly improves the timeline's readability. NVTX provides means to correlate the profile data with the application code. When profiling, you need to add `nvtx` to the tracing options of nsys profile. Example :  `nsys profile -t nvtx ./app`\n",
    "\n",
    "**Using NVTX with C/C++**:: For C/C++ code, add `#include \"nvtx3/nvToolsExt.h\"` in your source code and wrap parts of your code which you want to capture events with calls to the NVTX API functions. For example, try adding `nvtxRangePush(\"main\")` at the beginning of your `main()` function, and `nvtxRangePop(`) just before the return statement at the end. For more information, read https://github.com/NVIDIA/NVTX.\n",
    "\n",
    "The sample code snippet below shows the use of range events.The resulting NVTX markers can be viewed in Nsight Systems **Timeline** view. \n",
    "\n",
    "```cpp\n",
    "#include <nvtx3/nvToolsExt.h>\n",
    "...\n",
    "nvtxMark(\"Point in time\");\n",
    "...\n",
    "nvtxRangePush(\"Name of your code region\");\n",
    "// your code goes here\n",
    "nvtxRangePop();\n",
    "```\n",
    "\n",
    "**Using NVTX with Fortran**: The NVIDIA HPC SDK Fortran compiler provides NVTX bindings (`libnvhpcwrapnvtx.[a|so]` has to be linked). You would need to wrap parts of your code with `nvtxStartRange` and `nvtxEndRange` and add`-lnvhpcwrapnvtx` at the compile time . Documentation can be found here: https://docs.nvidia.com/hpc-sdk/compilers/fortran-cuda-interfaces/index.html#cfnvtx-runtime\n",
    "    \n",
    "```fortran\n",
    "use nvtx\n",
    "...\n",
    "call nvtxStartRange(\"YourRange\")\n",
    "! some Fortran code\n",
    "call nvtxEndRange\n",
    "```\n",
    "\n",
    "For other compilers, you can write your own Fortran NVTX bindings or use existing ones, e.g. https://raw.githubusercontent.com/maxcuda/NVTX_example/master/nvtx.f90\n",
    "\n",
    "**Using NVTX with Python**: Python developers can either use decorators `@nvtx.annotate()` or a context manager with `nvtx.annotate(..)`. To get NVTX Python Get NVTX Python module, use `python -m pip install nvtx`.\n",
    "\n",
    "\n",
    "```python\n",
    "import nvtx\n",
    "@nvtx.annotate(“f()”, color=\"purple\")\n",
    "def f():\n",
    "    for i in range(5):\n",
    "    with nvtx.annotate(\"loop\", color=\"red\"):\n",
    "    # Python code goes here\n",
    "```\n",
    "\n",
    "PyTorch CUDA provides NVTX bindings:\n",
    "\n",
    "```python\n",
    "from torch.cuda import nvtx\n",
    "\n",
    "nvtx.range_push(\"YourCode\")\n",
    "# your Python code\n",
    "nvtx.range_pop()\n",
    "```\n",
    "\n",
    "<img src=\"images/nvtx.PNG\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "You can learn more at https://nvtx.readthedocs.io/en/latest/index.html and https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/.\n",
    "\n",
    "\n",
    "Detailed NVTX documentation can be found under the __[CUDA Profiler user guide](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx)__.\n",
    "\n",
    "\n",
    "As well as tracing NVTX, CUDA (CUDA API trace and workload), Nsight Systems for Linux x86_64 and Power targets is capable of capturing information about OpenMP and OpenACC execution in the profiled process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# <div style=\"text-align: center ;border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\">[HOME](introduction.ipynb#steps)</div>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download the latest version of Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
